{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "author: Derryl Sayo\n",
        "bibliography: references.bib\n",
        "title: 'Linear Programming: Exhaustive Search vs. Simplex Method'\n",
        "include-in-header:\n",
        "  text: |\n",
        "    \\usepackage{fvextra}\n",
        "    \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from random import randint\n",
        "import numpy as np\n",
        "import itertools\n",
        "import scipy\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Linear Programming?\n",
        "Linear programming is defined by the standard form:\n",
        "\n",
        "$$\n",
        "\\begin{array}{rc}\n",
        "\\text{Maximize:} & \\mathbf{c}^T \\mathbf{x} \\\\\n",
        "\\text{Subject To:} & A \\mathbf{x} \\leq \\mathbf{b} \\\\\n",
        "& \\mathbf{x} \\geq 0\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Where $\\mathbf{c} \\in \\mathbb{R}^n$, $\\mathbf{x} \\in \\mathbb{R}^n$, $A$ is a $m \\times n$ matrix, $\\mathbf{b} \\in \\mathbb{R}^m$.\n",
        "\n",
        "The $x_i$ are refered to as **decision variables** whose values are to be decided and are restricted by the set of $m$ **constraints** of the form:\n",
        "\n",
        "$$\n",
        "a_{i,0} x_{i,0} + \\dots + a_{i,n-1} x_{i,n-1} \\begin{Bmatrix} \\geq \\\\ = \\\\ \\leq \\end{Bmatrix} b_i\n",
        "$$\n",
        "\n",
        "The **objective function** is a linear combination of the $n$ decision variables with weights $c_i$ written as:\n",
        "\n",
        "$$\n",
        "c_0x_0 + c_1x_1 + \\dots + c_{n-1}x_{n-1}\n",
        "$$\n",
        "\n",
        "\n",
        "In other words, linear programming is a way to model optimization problems with a linear objective function restricted by linear equality and inequality constraints. \n",
        "\n",
        "Solving an LP problem entails finding the set of decision variables that will lead to the maximum/minimum value of the objective function while satisfying all constraints.\n",
        "\n",
        "\n",
        "## Different Ways to Solve Linear Programming Problems\n",
        "The Fundamental Theorem of Linear Programming [see @wiki-ftlp] states that the optimal value of a bounded linear programming problem will occur at the intersection of $n$ constraint hyperplanes (ie. a vertex of the convex polytope enclosing the feasible set of solutions).\n",
        "\n",
        "At first glance, a naive approach would be to enumerate every single vertex, check each vertex's feasibility, and then calculate the max objective value over all vertices. However, as we will see later, this approach will run into complications as the scale of the problems increase.\n",
        "\n",
        "During the mid 1900s, linear programming became vital in the efforts of WW2 and ecomonics\n",
        "post war with many economists and mathematicians studying this class of problems independently [@Dantzig-Origins].\n",
        "The **simplex method**, developed by George Bernard Dantzig and detailed in _Linear Programming and Extensions_ [@Dantzig-LPE], arose as an efficient method to solve linear programming problems by iterating\n",
        "through adjacent feasible vertices until the optimal solution is reached.\n",
        "\n",
        "Other methods to solve linear programming problems include a family of interior-point methods which are not covered here.\n",
        "\n",
        "Below we will detail the first two methods for solving linear programming problems and compare the runtimes of each.\n",
        "\n",
        "### Geometry of Linear Programming\n",
        "Before we can iterate through the vertices, we first establish how we can programmatically find these vertices. \n",
        "\n",
        "LP problems have linear constraints which can be represented as hyperplanes of dimension $n - 1$ which make up a bounded polytope in $\\mathbb{R}^n$. A vertex of a polytope in $\\mathbb{R}^n$ is a point on its boundary at the intersection of $n$ hyperplanes.\n",
        "\n",
        "The optimal value of a bounded linear programming problem will be at one of these vertices. So, for each combination of $n$ constraints from the total pool of $n + m$ constraints (ie. $m$ constraints defined explicitly in $A$ and $n$ decision variable non-negativity constraints defined implicitly) we pick $n$ rows from $A$ and $b$ and solve $Ax = b$ for the intersection. If we do this for each $\\binom{n + m}{n}$ constraint intersections, we will have found every single vertex possible (feasible and infeasible) which we can then check for feasibility and then optimality.\n",
        "\n",
        "### Exhaustive Search by Enumerating All Vertices\n",
        "To be able to use linear programming solvers, we must create random LP problems. Below is a simple generator that returns the $A,b,c$ matrices/vectors with randomly selected dimensions $m$ and $n$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_random_LPP():\n",
        "        \"\"\"\n",
        "        Generate a random linear programming problem with `n` decision variables and `m` constraints.\n",
        "\n",
        "        Returns:\n",
        "        - A matrix `A` of LP constraints\n",
        "        - A vector `b` of LP constraint values\n",
        "        - The number of decision variables `n`\n",
        "        \"\"\"\n",
        "\n",
        "        # Number of constraints in the A matrix\n",
        "        m = randint(1, 12)\n",
        "\n",
        "        # Number of decision variables\n",
        "        n = randint(1, 12)\n",
        "\n",
        "        A = np.rint(np.random.rand(m, n) * 100)\n",
        "        b = np.rint(np.random.rand(m)  * 100)\n",
        "        c = np.rint(np.random.rand(n) * 100)\n",
        "\n",
        "        return A, b, c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we create a python function that will iterate through all feasible vertices and find the maximal objective value for the optimal solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_vertices(A, b):\n",
        "        \"\"\"\n",
        "        Find all vertices for an LP problem defined by A and b.\n",
        "\n",
        "        Params:\n",
        "        - A: m x n matrix of constraints (LHS of constraint inequalities)\n",
        "        - b: m x 1 matrix of constraint values (ie. RHS of constraint inequalities)\n",
        "\n",
        "        Returns:\n",
        "        A list of feasible vertices\n",
        "        \"\"\"\n",
        "\n",
        "        m = np.shape(A)[0]\n",
        "        n = np.shape(A)[1]\n",
        "\n",
        "        # Use itertools to get all n-tuple combinations from the m + n constraints\n",
        "        row_index_combinations = itertools.combinations(range(m + n), n)\n",
        "\n",
        "        # Since LP in standard form implicitly defines the x >= 0 constraint which corresponds to the x_i = 0 hyperplane\n",
        "        # So, we vstack the identity matrix onto A and hstack a vector of 0s to b to get\n",
        "        # all m + n constraint hyperplanes\n",
        "        A_stacked = np.vstack((A, np.eye(n)))\n",
        "        b_stacked = np.hstack((b, np.zeros(n)))\n",
        "\n",
        "        feasible_vertices = []\n",
        "\n",
        "        for row_combo in row_index_combinations:\n",
        "                # Select n constraints\n",
        "                A_constraints = A_stacked[list(row_combo), :]\n",
        "                b_constraints = b_stacked[list(row_combo)]\n",
        "\n",
        "                x = None\n",
        "\n",
        "                try:\n",
        "                  # Solve for the intersection of n constraint hyperplanes. If\n",
        "                  # we cannot solve due to a singular matrix, catch the error\n",
        "                  # and continue\n",
        "                  x = np.linalg.solve(A_constraints[:, :n], b_constraints)\n",
        "\n",
        "                  # Set too small values to 0 (account for floating point errors)\n",
        "                  x[(np.abs(x) < 1e-14)] = 0\n",
        "\n",
        "\n",
        "                  # Check that the vertex satisfies non-negative constraints and all inequality constraints\n",
        "                  # within a set tolerance\n",
        "                  if np.all(x >= 0) and np.all((A @ x) <= (b + 1e-10)):\n",
        "                    feasible_vertices.append(x)\n",
        "                except:\n",
        "                  pass\n",
        "\n",
        "        return feasible_vertices\n",
        "\n",
        "\n",
        "def LP_exhaustive_search(A, b, c):\n",
        "        \"\"\"\n",
        "        Solve the given linear programming problem by exhaustive search through all vertices.\n",
        "\n",
        "        Params:\n",
        "        - A: m x n matrix of constraints\n",
        "        - b: m x 1 vector of constraint inequality values\n",
        "        - c: 1 x n vector of objective function coefficients\n",
        "\n",
        "        Returns:\n",
        "        - Optimal objective value of min -c * x\n",
        "        \"\"\"\n",
        "        feasible_vertices = get_vertices(A, b)\n",
        "\n",
        "        objective_values = []\n",
        "\n",
        "        # Check the objective value for each of the found vertices\n",
        "        for vertex in feasible_vertices:\n",
        "                vertex_obj_val = np.dot(-c, vertex)\n",
        "                objective_values.append(vertex_obj_val)\n",
        "\n",
        "        return np.min(objective_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solving LP Problems by Simplex Method Using SciPy\n",
        "Detailed below is a python function which solves the given linear programming problem using `scipy.optimize.linprog` [@scipy_optimize_linprog] using the simplex method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def LP_simplex(A, b, c):\n",
        "        \"\"\"\n",
        "        Wrapper to call scipy.optimize.linprog with method=\"simplex\" for consistency\n",
        "        \"\"\"\n",
        "        return scipy.optimize.linprog(-c, A_ub=A, b_ub=b, method=\"simplex\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing Exhaustive Search vs. Simplex Method\n",
        "We can now run compare these two routines. Below we run 150 iterations of randomized LP problems of varying sizes and measure the runtimes of both the exhaustive search and the SciPy solver:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "\n",
        "num_iterations = 150\n",
        "\n",
        "scipy_test_results = []\n",
        "exhaustive_test_results = []\n",
        "\n",
        "for _ in range(1, num_iterations):\n",
        "    A, b, c = generate_random_LPP()\n",
        "    m, n = np.shape(A)\n",
        "\n",
        "    t_scipy = time.time()\n",
        "    scipy_res = LP_simplex(A, b, c)\n",
        "    duration_scipy = time.time() - t_scipy\n",
        "    scipy_test_results.append((m + n, duration_scipy))\n",
        "\n",
        "    t_exhaustive = time.time()\n",
        "    exhaustive_res = LP_exhaustive_search(A, b, c)\n",
        "    duration_exhaustive = time.time() - t_exhaustive\n",
        "    exhaustive_test_results.append((m + n, duration_exhaustive))\n",
        "\n",
        "    # Check that the results are the same to 5 decimal places\n",
        "    assert(np.round(scipy_res.fun, 5) == np.round(exhaustive_res, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Exhaustive Search vs. Simplex Method\n",
        "Below is a figure comparing the runtimes between exhaustive search and the simplex method for problems of size $m + n$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x1, y1 = zip(*scipy_test_results)\n",
        "x2, y2 = zip(*exhaustive_test_results)\n",
        "\n",
        "plt.scatter(x1, y1, label=\"SciPy LP Solver\", marker=\"x\")\n",
        "plt.scatter(x2, y2, label=\"Exhaustive Search LP Solver\", marker=\"x\")\n",
        "\n",
        "plt.xlabel(\"m + n\")\n",
        "plt.ylabel(\"Runtime (s)\")\n",
        "plt.title(\"Exhaustive Search vs. SciPy Simplex Method Runtimes\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the current implementation, the exhaustive search method's runtime grows exponentially as the problem size grows and already takes a non-trivial amount of time for small problems of size $n + m \\approx 25$. Meanwhile, the simplex method remains efficient and doesn't grow rapidly.\n",
        "\n",
        "## Runtime Analysis\n",
        "In order to compare the efficiency of these two algorithms we first need to establish some notion of quantifiable measures and distinguish between worst case and average case runtimes. Oftentimes, we use\n",
        "some function on the number of operations required in a given routine.\n",
        "\n",
        "In the case of linear programming solvers, we know that the optimal value exists at a vertex of the feasible region and that the basis\n",
        " of the simplex algorithm is iterating through adjacent vertices with increasing objection function value so a sensible measure would be to see how much many vertices we must check before arriving at the\n",
        " optimal solution.\n",
        "\n",
        "### Worst Case Time\n",
        "In regards to the simplex method/other forms of vertex based solvers, the \"worst case\" runtime can be thought of as \"if I take the longest route possible, how many vertices must I pass through\"? For a problem\n",
        "with $m$ constraints and $n$ decision variables, each vertex exists as the intersection of $n$ constraint hyperplanes. So, the number of vertices is $n + m$ choose $n$, or:\n",
        "\n",
        "$$\n",
        "\\binom{n+m}{n} = \\frac{(n + m)!}{n!m!}\n",
        "$$\n",
        "\n",
        "Written as the fraction of factorials, it becomes clear that this grows exponentially relative to problem size. For example, a problem with $12$ constraints and $10$ decision variables in the worst-case needs\n",
        " to check $\\binom{22}{10} = 646646$ vertices. For non-trivial LP problems, this number quickly becomes too large to viably search through every vertex until optimality is reached.\n",
        "\n",
        "### Average Case Time\n",
        "As seen in the figure above, practical LP solvers' runtime does not grow exponentially even though the theoretical worst-case runtime does grow exponentially. Explanations may include that the exhaustive \n",
        "search implementation detailed above requires solving a linear system of equations for every $n + m$ choose $n$ vertices without extra heurstics to trim vertices outside the feasible regions compared to the actual \n",
        "geometry of the problem allowing for very few pivots to reach optimality.\n",
        "\n",
        "Further research has gone into why the simplex algorithm performs well under most circumstances. In \"Smoothed Analysis of Algorithms: Why the Simplex Algorithm Usually Takes Polynomial Time\" [@spielman2003smoothed], Spielman and Teng have shown that under \"smoothed analysis\", \n",
        "the simplex method has polynomial-time complexity rather than exponential.\n",
        "\n",
        "## Conclusion\n",
        "Above, we have detailed code to generate and solve linear programming problems using exhaustive search through all vertices and have seen that the runtime grows exponentially \n",
        "when compared to the more stable simplex algorithm. Althrough both algorithms theoretically have the same worst case, recent analysis has\n",
        " shown why the simplex algorithm usually only takes polynomial time.\n",
        "\n",
        "When reading further into this topic, future topics of interest include a more in-depth look into the smoothed analysis\n",
        " paper on the practical efficiency of the simplex algorithm, interior point LP solvers, and the Klee-Minty cube for a problem designed to \n",
        "demonstrate worst-case performance of the simplex algorithm.\n",
        "\n",
        "## References\n",
        "```{bibliography}\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}